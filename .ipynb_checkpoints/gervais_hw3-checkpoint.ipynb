{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "WORLD_SIZE = 5\n",
    "A_POS = [0, 1]\n",
    "A_PRIME_POS = [4, 1]\n",
    "B_POS = [0, 3]\n",
    "B_PRIME_POS = [2, 3]\n",
    "discount = 0.9\n",
    "\n",
    "world = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "\n",
    "# left, up, right, down\n",
    "actions = ['L', 'U', 'R', 'D']\n",
    "\n",
    "actionProb = []\n",
    "for i in range(0, WORLD_SIZE):\n",
    "    actionProb.append([])\n",
    "    for j in range(0, WORLD_SIZE):\n",
    "        actionProb[i].append(dict({'L':0.25, 'U':0.25, 'R':0.25, 'D':0.25}))\n",
    "\n",
    "nextState = []\n",
    "actionReward = []\n",
    "for i in range(0, WORLD_SIZE):\n",
    "    nextState.append([])\n",
    "    actionReward.append([])\n",
    "    for j in range(0, WORLD_SIZE):\n",
    "        next = dict()\n",
    "        reward = dict()\n",
    "        if i == 0:\n",
    "            next['U'] = [i, j]\n",
    "            reward['U'] = -1.0\n",
    "        else:\n",
    "            next['U'] = [i - 1, j]\n",
    "            reward['U'] = 0.0\n",
    "\n",
    "        if i == WORLD_SIZE - 1:\n",
    "            next['D'] = [i, j]\n",
    "            reward['D'] = -1.0\n",
    "        else:\n",
    "            next['D'] = [i + 1, j]\n",
    "            reward['D'] = 0.0\n",
    "\n",
    "        if j == 0:\n",
    "            next['L'] = [i, j]\n",
    "            reward['L'] = -1.0\n",
    "        else:\n",
    "            next['L'] = [i, j - 1]\n",
    "            reward['L'] = 0.0\n",
    "\n",
    "        if j == WORLD_SIZE - 1:\n",
    "            next['R'] = [i, j]\n",
    "            reward['R'] = -1.0\n",
    "        else:\n",
    "            next['R'] = [i, j + 1]\n",
    "            reward['R'] = 0.0\n",
    "\n",
    "        if [i, j] == A_POS:\n",
    "            next['L'] = next['R'] = next['D'] = next['U'] = A_PRIME_POS\n",
    "            reward['L'] = reward['R'] = reward['D'] = reward['U'] = 10.0\n",
    "\n",
    "        if [i, j] == B_POS:\n",
    "            next['L'] = next['R'] = next['D'] = next['U'] = B_PRIME_POS\n",
    "            reward['L'] = reward['R'] = reward['D'] = reward['U'] = 5.0\n",
    "\n",
    "        nextState[i].append(next)\n",
    "        actionReward[i].append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#generate a random trajectory for gridworld\n",
    "def generate_episode(x, y, n_moves):\n",
    "    #intialize traj with a state, action, and reward for each iteration in number of moves.\n",
    "    traj = [{'state': [0,0], 'action': '', 'reward': 0.0} for i in range(n_moves)]\n",
    "    #loop running as many times specified by n_moves\n",
    "    for i in range(n_moves):\n",
    "        #set the state of this trajectory iteration\n",
    "        traj[i]['state'] = [y,x]\n",
    "        #pick a random action for this trajectory iteration\n",
    "        cur_act = random.choice(['L', 'U', 'R', 'D'])\n",
    "        #set the action\n",
    "        traj[i]['action'] = cur_act\n",
    "        #calculate and set the reward from this state and action\n",
    "        traj[i]['reward'] = actionReward[y][x][cur_act]\n",
    "        #find the next state from the previous state and the action taken\n",
    "        y,x = nextState[y][x][cur_act]\n",
    "    return traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcReward(trajectory):\n",
    "    reward = 0\n",
    "    #loop through trajectory starting at index j, where we found the first occurence\n",
    "    for s in range(j, len(trajectory)):\n",
    "        #sum all the rewards coming after s_j\n",
    "        gamma = 0.9\n",
    "        #with gamma\n",
    "        #reward += (gamma**((s-j)))*trajectory[s]['reward']\n",
    "        #without gamma\n",
    "        reward += trajectory[s]['reward']\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First-visit MC prediction\n",
    "\n",
    "def firstVisitMCPrediction(num_episodes):\n",
    "    value_state = [[0 for x in range(5)] for y in range(5)]\n",
    "    #iterate through the number of episodes given\n",
    "    for i in range(num_episodes):\n",
    "        #get random starting point on gridworld map\n",
    "        start_y = random.randint(0,4)\n",
    "        start_x = random.randint(0,4)\n",
    "        #generate episode trajectory with 100 random movies\n",
    "        t = generate_episode(start_y, start_x, 100)\n",
    "        \n",
    "        #iterate through all possible states\n",
    "        for y in range(5):\n",
    "            for x in range(5):\n",
    "                #current state = s_j\n",
    "                s_j = [y,x]\n",
    "                #iterate through the trajectory to find the first occurence of the current state (y, x)\n",
    "                for j in range(len(t)):\n",
    "                    #found the first occurence of s_j in t\n",
    "                    if s_j == t[j]['state']:\n",
    "                        #calculate the total reward proceeding this state.\n",
    "                        reward = mcReward(t)\n",
    "\n",
    "                        #i+1 is the number of episode that have occured before this episode including this episode\n",
    "                    \n",
    "                        #update the value state for this state, (start_y, start_x)\n",
    "                        value_state[y][x] = value_state[y][x] + (1/(i+1)) * (reward - value_state[y][x])\n",
    "    return value_state  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.07669428  5.58571902  3.95773385  4.58189973 -1.9884335 ]\n",
      " [ 1.70558857  1.61139807  1.11991848  1.88265438 -2.05329679]\n",
      " [-1.8975151  -1.55383546 -0.75305341 -0.33490099 -1.79062979]\n",
      " [-1.69949553 -1.72362614 -2.48789646 -2.97364721 -3.670344  ]\n",
      " [-2.32045415 -1.5388675  -3.07750769 -4.32475979 -5.54331106]]\n"
     ]
    }
   ],
   "source": [
    "#run the first visit monte carlo prediction with 100 episodes\n",
    "random\n",
    "print(np.asarray(firstVisitMCPrediction(100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
